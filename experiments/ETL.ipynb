{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from random import randint\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from functools import partial\n",
    "import pickle\n",
    "import ast\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "punctuation_without_dot = list(filter(lambda x: x != '.' or x != ',' , string.punctuation))\n",
    "STOPWORDS = list(set(stopwords.words('english'))) + punctuation_without_dot\n",
    "#WARNING ! ! ! \n",
    "#There are several known issues with ‘english’ and you should consider an alternative.\n",
    "#-> https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "\n",
    "import data_lake_helper as dl_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lake = dl_helper.DataLake(version='v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(feature, load_version=None):\n",
    "\n",
    "    if load_version is None:\n",
    "        data_lake_ = data_lake\n",
    "    else:\n",
    "        data_lake_ = DataLake(version=load_version)\n",
    "    \n",
    "    df[feature] = data_lake_.load_obj(feature + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up dataframe \n",
    "\n",
    "df_train_table = pd.read_csv('dataset/train_set.csv')\n",
    "df_train_table.index = df_train_table.pk_id\n",
    "df_train_table['path'] = 'dataset/train_set/'\n",
    "del df_train_table['pk_id']\n",
    "\n",
    "df_test_table = pd.read_csv('dataset/test_set.csv')\n",
    "df_test_table.index = df_test_table.pk_id\n",
    "df_test_table['path'] = 'dataset/test_set/'\n",
    "del df_test_table['pk_id']\n",
    "\n",
    "df = df_train_table.append(df_test_table, sort=False)\n",
    "\n",
    "del df['master_tree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.values = []\n",
    "        self.name = ''\n",
    "    \n",
    "    def transform(self, text):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'letter_lenght'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Count(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'word_count'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumeration_regex = \"\\n[0-9]{1,2}[.]\\s[a-zA-Z]\"\n",
    "enumeration_regex_2 = \"\\n[0-9]{1,2}[.]\"\n",
    "\n",
    "class Enum_Presence(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'enumeration_presence'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        enums = re.findall(enumeration_regex, text)\n",
    "        enums_len = len(enums)\n",
    "        return enums_len > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enum_Count(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'enumeration_count'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        enums = re.findall(enumeration_regex, text)\n",
    "        enums_len = len(enums)\n",
    "        return enums_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enum_Repeated(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'enumeration_repeated'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        enums = re.findall(enumeration_regex, text)\n",
    "        enums = re.findall(enumeration_regex_2, ' '.join(enums))\n",
    "        \n",
    "        unique_enums = list(set(enums))\n",
    "        has_repeated_enums = not len(unique_enums) == len(enums)\n",
    "        return has_repeated_enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_for_text_list(text_list, text):\n",
    "    text = text.lower()\n",
    "    has_text = False\n",
    "    for text_to_look_for in text_list:\n",
    "        has_text = (text_to_look_for in text) or has_text\n",
    "    return has_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEC letters classification\n",
    "################################\n",
    "\n",
    "class SEC_Header(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'sec_header_presence'\n",
    "        self.values = []\n",
    "        \n",
    "        header_sentence_1 = 'UNITED STATES SECURITIES AND EXCHANGE COMMISSION'.lower()\n",
    "        header_sentence_2 = 'WASHINGTON, D.C. 20549-4628'.lower()\n",
    "        header_sentence_3 = 'DIVISION OF CORPORATION FINANCE'.lower()\n",
    "        self.sec_headers = [header_sentence_1, header_sentence_2, header_sentence_3]\n",
    "        self.sec_headers = ' '.join(sec_headers)\n",
    "\n",
    "        self.look_for_sec_headers = partial(look_for_text_list, sec_headers)\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return look_for_sec_headers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Response letters classification\n",
    "################################\n",
    "\n",
    "class Response_Text(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'response_presence'\n",
    "        self.values = []\n",
    "        \n",
    "        rl_sentence_2 = 'response :'.lower()\n",
    "        rl_sentence_4 = 'responses :'.lower()\n",
    "        rl_headers = [rl_sentence_2, rl_sentence_4]\n",
    "        self.look_for_rl_headers = partial(look_for_text_list, rl_headers)\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return look_for_rl_headers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment letters classification\n",
    "################################\n",
    "\n",
    "class Comment_Text(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'cl_sentence_presence'\n",
    "        self.values = []\n",
    "        \n",
    "        cl_sentence_1 = 'We have reviewed your filing and have the following comments.'.lower()\n",
    "        cl_sentence_2 = 'After reviewing your response to these comments, we may have additional comments.'.lower()\n",
    "        cl_headers = [cl_sentence_1, cl_sentence_2]\n",
    "\n",
    "        self.look_for_cl_headers = partial(look_for_text_list, cl_headers)\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return look_for_cl_headers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension = '.txt'\n",
    "\n",
    "features = [Length(), \n",
    "            Word_Count(),\n",
    "            Enum_Presence(),\n",
    "            Enum_Count(),\n",
    "            Enum_Repeated(),\n",
    "            SEC_Header(),\n",
    "            Response_Text(),\n",
    "            Comment_Text()]\n",
    "\n",
    "texts = []\n",
    "for i in range(0,len(df)):\n",
    "    item = df.iloc[i]\n",
    "    pk_id = str(item.name)\n",
    "    text = open(item.path + pk_id + file_extension).read()\n",
    "    texts.append(text)\n",
    "    \n",
    "    #getting each feature from every text item\n",
    "    for feature in features:\n",
    "        t = feature.transform(text)\n",
    "        feature.values.append(t)    \n",
    "\n",
    "#saving each feature as a pandas series\n",
    "for feature in features:\n",
    "    df[feature.name] = feature.values\n",
    "    feature_as_serie = df[feature.name]\n",
    "    data_lake.save_obj(feature_as_serie, feature.name + '.pkl')\n",
    "    del df[feature.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting word_density feature here!\n",
    "#\n",
    "letter_lenght = data_lake.load_obj(Length().name + '.pkl')\n",
    "word_count = data_lake.load_obj(Word_Count().name + '.pkl')\n",
    "word_density = letter_lenght/(word_count+1)\n",
    "\n",
    "data_lake.save_obj(word_density, 'word_density.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter_lenght 55556\n",
      "word_count 55556\n",
      "enumeration_presence 55556\n",
      "enumeration_count 55556\n",
      "enumeration_repeated 55556\n",
      "sec_header_presence 55556\n",
      "response_presence 55556\n",
      "cl_sentence_presence 55556\n",
      "df 55556\n"
     ]
    }
   ],
   "source": [
    "#just checking...\n",
    "\n",
    "for f in features:\n",
    "    print(f.name, len(data_lake.load_obj(f.name + '.pkl')))\n",
    "\n",
    "print('df', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToLower(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        return [text.lower() for text in texts]\n",
    "        \n",
    "class TextWithOutStopWords(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        texts_ = []\n",
    "        for text in texts:\n",
    "            text = text.replace('\\n', ' ')\n",
    "            text = \" \".join([i for i in word_tokenize(text) if i not in STOPWORDS])\n",
    "            texts_.append(text)\n",
    "        \n",
    "        return texts_\n",
    "\n",
    "class TextWithOutNonASCII(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        texts_ = []\n",
    "        for text in texts:\n",
    "            text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "            texts_.append(text)\n",
    "        \n",
    "        return texts_\n",
    "    \n",
    "ps = PorterStemmer() \n",
    "class TextStemming(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        texts_ = []\n",
    "        for text in texts:\n",
    "            tokenized_words = word_tokenize(text)\n",
    "            for i, w in enumerate(tokenized_words):\n",
    "                tokenized_words[i] = ps.stem(w)\n",
    "                text = ' '.join(tokenized_words)\n",
    "            \n",
    "            texts_.append(text)\n",
    "        \n",
    "        return texts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLake' object has no attribute 'save_object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-2dc80bbf0553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata_lake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dataset/train_set/'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLake' object has no attribute 'save_object'"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"T_to_L\", TextToLower()),\n",
    "    (\"T_no_SW\", TextWithOutStopWords()),\n",
    "    (\"T_no_ascii\", TextWithOutNonASCII()),\n",
    "    (\"TS\", TextStemming())#PROBAR TMB CON Lemmatization !!!\n",
    "    ])\n",
    "    \n",
    "f_name = 'text_normalized'\n",
    "texts_t = pipeline.transform(texts)\n",
    "\n",
    "df[f_name] = texts_t\n",
    "data_lake.save_obj(df[f_name], f_name + '.pkl')\n",
    "del df[f_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_feature('letter_lenght')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'letter_lenght distribution')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df.letter_lenght, bins = 1000)\n",
    "plt.xlim(0, 60000)\n",
    "plt.ylabel('density')\n",
    "plt.title('letter_lenght distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_.letter_lenght > 35000 => 4.02\n",
      "df_.letter_lenght > 35000 => ['RL' 'CL' 'UU']\n"
     ]
    }
   ],
   "source": [
    "threshold = 35000\n",
    "msg = \"df_.letter_lenght > \" + str(threshold) + \" =>\"\n",
    "print(msg, round(len(df[df.letter_lenght > threshold]) / len(df) * 100, 2))\n",
    "print(msg, df[df.letter_lenght > threshold].category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers\n",
    "df = df[df.letter_lenght < threshold]\n",
    "df_train_table = df[df.path == 'dataset/train_set/']\n",
    "df_test_table = df[df.path == 'dataset/test_set/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorias que no se puede entrenar:\n",
      "['ACAK']\n"
     ]
    }
   ],
   "source": [
    "# SHOULD REMOVE ACAK CATEGORY !!!\n",
    "#\n",
    "\n",
    "train_categories = df_train_table.category.unique()\n",
    "test_categories = df_test_table.category.unique()\n",
    "\n",
    "if len(train_categories) < len(test_categories):\n",
    "    print('categorias que no se puede entrenar:')\n",
    "    cat_to_remove = [c for c in test_categories if c not in train_categories]\n",
    "    print(cat_to_remove)\n",
    "\n",
    "    #removing categories that are not in the train data set\n",
    "    for c in cat_to_remove:\n",
    "        df = df[df.category != c]\n",
    "        df_train_table = df[df.path == 'dataset/train_set/']\n",
    "        df_test_table = df[df.path == 'dataset/test_set/']\n",
    "else:\n",
    "    print('train & test dataset presentan las mismas categorias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['letter_lenght']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving cleaned dataframe !!\n",
    "#\n",
    "\n",
    "data_lake.save_obj(df, 'df-cleaned.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_feature(f_name)\n",
    "train_x = df[df.path == 'dataset/train_set/'][f_name]\n",
    "valid_x = df[df.path == 'dataset/test_set/'][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting CountVectorizer...\n",
      "fitting finished - time:  124.39757490158081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Vectors as features\n",
    "########################################\n",
    "\n",
    "print(\"fitting CountVectorizer...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('count_vect_config.txt')\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer=config['analyzer'],\n",
    "                             token_pattern=config['token_pattern'],\n",
    "                             ngram_range=config['ngram_range'])\n",
    "count_vect.fit(df[f_name])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "#saving matrices\n",
    "data_lake.save_npz(xvalid_count, \"xvalid_count.npz\")\n",
    "data_lake.save_npz(xtrain_count, \"xtrain_count.npz\")\n",
    "data_lake.save_obj(count_vect, \"count_vect.pkl\")\n",
    "\n",
    "print(\"fitting finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting word level tf-idf...\n",
      "fitting finished - time:  47.41982913017273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "###################\n",
    "\n",
    "print(\"fitting word level tf-idf...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('tf_idf_word_vect_config.txt')\n",
    "\n",
    "#TODO: probar otros valores para max_features, 1D uso 50.000\n",
    "tfidf_vect = TfidfVectorizer(analyzer=config['analyzer'],\n",
    "                             token_pattern=config['token_pattern'],\n",
    "                             max_features=config['max_features'])\n",
    "tfidf_vect.fit(df[f_name])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "#saving matrices\n",
    "data_lake.save_npz(xvalid_tfidf, \"xvalid_tfidf.npz\")\n",
    "data_lake.save_npz(xtrain_tfidf, \"xtrain_tfidf.npz\")\n",
    "\n",
    "print(\"fitting finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting ngram level tf-idf...\n",
      "fitting finished - time:  246.00837469100952\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n-gram level tf-idf \n",
    "####################\n",
    "\n",
    "print(\"fitting ngram level tf-idf...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('tf_idf_n_gram_vect_config.txt')\n",
    "\n",
    "#TODO: probar otros valores para max_features, 1D uso 50.000\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer=config['analyzer'],\n",
    "                                   token_pattern=config['token_pattern'],\n",
    "                                   ngram_range=config['ngram_range'],\n",
    "                                   max_features=config['max_features'])\n",
    "tfidf_vect_ngram.fit(df[f_name])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "#saving matrices\n",
    "data_lake.save_npz(xvalid_tfidf_ngram, \"xvalid_tfidf_ngram.npz\")\n",
    "data_lake.save_npz(xtrain_tfidf_ngram, \"xtrain_tfidf_ngram.npz\")\n",
    "data_lake.save_obj(tfidf_vect_ngram, \"tfidf_vect_ngram.pkl\")\n",
    "\n",
    "print(\"fitting finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting characters level tf-idf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting finished - time:  443.69289684295654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# characters level tf-idf\n",
    "#########################\n",
    "\n",
    "print(\"fitting characters level tf-idf...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('tf_idf_char_vect_config.txt')\n",
    "\n",
    "#TODO: probar otros valores para max_features, 1D uso 50.000\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer=config['analyzer'],\n",
    "                                         token_pattern=config['token_pattern'],\n",
    "                                         ngram_range=config['ngram_range'],\n",
    "                                         max_features=config['max_features'])\n",
    "tfidf_vect_ngram_chars.fit(df[f_name])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "#saving matrices\n",
    "data_lake.save_npz(xvalid_tfidf_ngram_chars, \"xvalid_tfidf_ngram_chars\" + \".npz\")\n",
    "data_lake.save_npz(xtrain_tfidf_ngram_chars, \"xtrain_tfidf_ngram_chars\" + \".npz\")\n",
    "\n",
    "print(\"fitting finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LDA Model...\n",
      "training finished - time:  899.3639438152313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Topic Models as features\n",
    "####################################\n",
    "\n",
    "print(\"training LDA Model...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('lda_config.txt')\n",
    "\n",
    "# train a LDA Model\n",
    "#TODO: SHOULD TRY VALUES FOR n_components !!!\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=config['n_components'],\n",
    "                                                    learning_method=config['learning_method'],\n",
    "                                                    max_iter=config['max_iter'])\n",
    "X_topics = lda_model.fit_transform(xtrain_tfidf_ngram)\n",
    "\n",
    "data_lake.save_obj(X_topics, 'X_topics' + '.pkl')\n",
    "data_lake.save_obj(lda_model, 'lda_model' + '.pkl')\n",
    "\n",
    "print(\"training finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
