{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy import sparse\n",
    "import ast\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras import layers, models, optimizers\n",
    "from keras.layers import Input, Dense, AlphaDropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from sklearn import model_selection, preprocessing, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import pickle\n",
    "import textblob, string\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "import data_lake_helper as dl_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lake = dl_helper.DataLake(version='v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_to_prepare, key='text_normalized'):\n",
    "    train_x = df_to_prepare[key].tolist()\n",
    "    train_y = df_to_prepare.category.tolist()\n",
    "    return (train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(feature, load_version=None):\n",
    "\n",
    "    if load_version is None:\n",
    "        data_lake_ = data_lake\n",
    "    else:\n",
    "        data_lake_ = DataLake(version=load_version)\n",
    "    \n",
    "    df[feature] = data_lake_.load_obj(feature + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_lake.load_obj('df-cleaned.pkl')\n",
    "\n",
    "f_name = 'text_normalized'\n",
    "load_feature(f_name)\n",
    "\n",
    "df_train_table = df[df.path == 'dataset/train_set/']\n",
    "df_test_table = df[df.path == 'dataset/test_set/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = prepare_data(df_train_table)\n",
    "valid_x, valid_y = prepare_data(df_test_table)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelBinarizer()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_types = sorted(df.category.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing feature that we wont't use anymore\n",
    "del df[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = data_lake.load_config('snn_config.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_BREAK = '\\n'\n",
    "class BLCounter(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        return [[text.count(LINE_BREAK)] for text in texts]\n",
    "\n",
    "class TextCuter(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        return [text[:15000] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    train_x = data_lake.load_npz('snn_xtrain_vect.npz')\n",
    "    valid_x = data_lake.load_npz('snn_xvalid_vect.npz')\n",
    "\n",
    "except:\n",
    "    vectorizer = FeatureUnion([\n",
    "      (\"c_vect\", Pipeline([\n",
    "           (\"TC\", TextCuter()),\n",
    "           (\"CV\", CountVectorizer(ngram_range=model_params['ngram_range'], \n",
    "                                  max_features=model_params['max_features'],\n",
    "                                  stop_words=model_params['letters_language'])),\n",
    "           (\"SS\", StandardScaler(with_mean=False))])),\n",
    "      (\"len\", Pipeline([(\"BC\", BLCounter()), (\"SS\", StandardScaler())]))])\n",
    "\n",
    "    train_x = vectorizer.fit_transform(df_train_table[f_name])\n",
    "    valid_x = vectorizer.transform(df_test_table[f_name])\n",
    "    \n",
    "    data_lake.save_npz(train_x, 'snn_xtrain_vect.npz')\n",
    "    data_lake.save_npz(valid_x, 'snn_xvalid_vect.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Sequential()\n",
    "pipeline.add(AlphaDropout(model_params['input_layer_dropout'], input_shape=(model_params['max_features'] + 1,)))\n",
    "\n",
    "for _ in range(model_params['n_hidden_layers']):\n",
    "    pipeline.add(Dense(model_params['neurons_per_layer'], \n",
    "                        activation=model_params['hidden_layers_activation'],\n",
    "                        bias_initializer=model_params['neurons_initializer'],\n",
    "                        kernel_initializer=model_params['neurons_initializer']))\n",
    "    pipeline.add(AlphaDropout(model_params['hidden_layer_dropout']))\n",
    "\n",
    "pipeline.add(Dense(len(letter_types), \n",
    "                    bias_initializer=model_params['neurons_initializer'],\n",
    "                    kernel_initializer=model_params['neurons_initializer'],\n",
    "                    activation=model_params['output_layer_activation']))\n",
    "\n",
    "pipeline.compile(optimizer=Adam(lr=model_params['optimizer_learning_rate'],\n",
    "                                 decay=model_params['optimizer_learning_decay']),\n",
    "                  loss=model_params['training_loss'], metrics=model_params['training_metrics_list'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "\n",
    "try:\n",
    "    pipeline = data_lake.load_obj('snn_model.pkl')\n",
    "\n",
    "except:\n",
    "    print(\"fitting SNN...\")\n",
    "    time_start = time.time()\n",
    "\n",
    "    pipeline.fit(\n",
    "        train_x, train_y,\n",
    "        epochs=EPOCHS, batch_size=128, validation_split = 0.2, verbose=1)\n",
    "\n",
    "    data_lake.save_obj(pipeline, 'snn_model.pkl')\n",
    "    \n",
    "    print(\"fitting finished - time: \", time.time() - time_start)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECALL\n",
      "('AK', 0.7391304347826086)\n",
      "('AR', 1.0)\n",
      "('CL', 0.9967845659163987)\n",
      "('ER', 0.8181818181818182)\n",
      "('FL', 1.0)\n",
      "('ND', 0.0)\n",
      "('RL', 0.9854651162790697)\n",
      "('TM', 0.75)\n",
      "('UR', 1.0)\n",
      "('UU', 0.0)\n",
      "\n",
      "ACCURACY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8959913326110509"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pipeline.predict(valid_x)\n",
    "\n",
    "items_recall = recall_score(valid_y.argmax(-1), score.argmax(axis=-1), average=None)\n",
    "\n",
    "print('RECALL')\n",
    "for item in zip(letter_types,items_recall):\n",
    "    print(str(item))\n",
    "    \n",
    "print()\n",
    "print('ACCURACY')\n",
    "metrics.accuracy_score(score.argmax(-1), valid_y.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
