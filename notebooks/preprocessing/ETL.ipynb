{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from random import randint\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from functools import partial\n",
    "import pickle\n",
    "import ast\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "punctuation_without_dot = list(filter(lambda x: x != '.' or x != ',' , string.punctuation))\n",
    "STOPWORDS = list(set(stopwords.words('english'))) + punctuation_without_dot\n",
    "#WARNING ! ! ! \n",
    "#There are several known issues with ‘english’ and you should consider an alternative.\n",
    "#-> https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "\n",
    "import data_lake_helper as dl_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lake = dl_helper.DataLake(version='v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(feature, load_version=None):\n",
    "\n",
    "    if load_version is None:\n",
    "        data_lake_ = data_lake\n",
    "    else:\n",
    "        data_lake_ = DataLake(version=load_version)\n",
    "    \n",
    "    df[feature] = data_lake_.load_obj(feature + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up dataframe \n",
    "\n",
    "df_train_table = pd.read_csv('../../source/dataset/train_set.csv')\n",
    "df_train_table.index = df_train_table.pk_id\n",
    "df_train_table['path'] = 'source/dataset/train_set/'\n",
    "del df_train_table['pk_id']\n",
    "\n",
    "df_test_table = pd.read_csv('../../source/dataset/test_set.csv')\n",
    "df_test_table.index = df_test_table.pk_id\n",
    "df_test_table['path'] = 'source/dataset/test_set/'\n",
    "del df_test_table['pk_id']\n",
    "\n",
    "df = df_train_table.append(df_test_table, sort=False)\n",
    "\n",
    "del df['master_tree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.values = []\n",
    "        self.name = ''\n",
    "    \n",
    "    def transform(self, text):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'letter_lenght'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word_Count(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'word_count'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumeration_regex = \"\\n[0-9]{1,2}[.]\\s[a-zA-Z]\"\n",
    "enumeration_regex_2 = \"\\n[0-9]{1,2}[.]\"\n",
    "\n",
    "class Enum_Presence(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'enumeration_presence'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        enums = re.findall(enumeration_regex, text)\n",
    "        enums_len = len(enums)\n",
    "        return enums_len > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enum_Count(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'enumeration_count'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        enums = re.findall(enumeration_regex, text)\n",
    "        enums_len = len(enums)\n",
    "        return enums_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enum_Repeated(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'enumeration_repeated'\n",
    "        self.values = []\n",
    "    \n",
    "    def transform(self, text):\n",
    "        enums = re.findall(enumeration_regex, text)\n",
    "        enums = re.findall(enumeration_regex_2, ' '.join(enums))\n",
    "        \n",
    "        unique_enums = list(set(enums))\n",
    "        has_repeated_enums = not len(unique_enums) == len(enums)\n",
    "        return has_repeated_enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_for_text_list(text_list, text):\n",
    "    text = text.lower()\n",
    "    has_text = False\n",
    "    for text_to_look_for in text_list:\n",
    "        has_text = (text_to_look_for in text) or has_text\n",
    "    return has_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEC letters classification\n",
    "################################\n",
    "\n",
    "class SEC_Header(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'sec_header_presence'\n",
    "        self.values = []\n",
    "        \n",
    "        header_sentence_1 = 'UNITED STATES SECURITIES AND EXCHANGE COMMISSION'.lower()\n",
    "        header_sentence_2 = 'WASHINGTON, D.C. 20549-4628'.lower()\n",
    "        header_sentence_3 = 'DIVISION OF CORPORATION FINANCE'.lower()\n",
    "        self.sec_headers = [header_sentence_1, header_sentence_2, header_sentence_3]\n",
    "        self.sec_headers = ' '.join(sec_headers)\n",
    "\n",
    "        self.look_for_sec_headers = partial(look_for_text_list, sec_headers)\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return look_for_sec_headers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Response letters classification\n",
    "################################\n",
    "\n",
    "class Response_Text(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'response_presence'\n",
    "        self.values = []\n",
    "        \n",
    "        rl_sentence_2 = 'response :'.lower()\n",
    "        rl_sentence_4 = 'responses :'.lower()\n",
    "        rl_headers = [rl_sentence_2, rl_sentence_4]\n",
    "        self.look_for_rl_headers = partial(look_for_text_list, rl_headers)\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return look_for_rl_headers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment letters classification\n",
    "################################\n",
    "\n",
    "class Comment_Text(Feature):\n",
    "    def __init__(self):\n",
    "        self.name = 'cl_sentence_presence'\n",
    "        self.values = []\n",
    "        \n",
    "        cl_sentence_1 = 'We have reviewed your filing and have the following comments.'.lower()\n",
    "        cl_sentence_2 = 'After reviewing your response to these comments, we may have additional comments.'.lower()\n",
    "        cl_headers = [cl_sentence_1, cl_sentence_2]\n",
    "\n",
    "        self.look_for_cl_headers = partial(look_for_text_list, cl_headers)\n",
    "    \n",
    "    def transform(self, text):\n",
    "        return look_for_cl_headers(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_extension = '.txt'\n",
    "\n",
    "features = [Length(), \n",
    "            Word_Count(),\n",
    "            Enum_Presence(),\n",
    "            Enum_Count(),\n",
    "            Enum_Repeated(),\n",
    "            SEC_Header(),\n",
    "            Response_Text(),\n",
    "            Comment_Text()]\n",
    "\n",
    "texts = []\n",
    "for i in range(0,len(df)):\n",
    "    item = df.iloc[i]\n",
    "    pk_id = str(item.name)\n",
    "    text = open(item.path + pk_id + file_extension).read()\n",
    "    texts.append(text)\n",
    "    \n",
    "    #getting each feature from every text item\n",
    "    for feature in features:\n",
    "        t = feature.transform(text)\n",
    "        feature.values.append(t)    \n",
    "\n",
    "#saving each feature as a pandas series\n",
    "for feature in features:\n",
    "    df[feature.name] = feature.values\n",
    "    feature_as_serie = df[feature.name]\n",
    "    data_lake.save_obj(feature_as_serie, feature.name + '.pkl')\n",
    "    del df[feature.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting word_density feature here!\n",
    "#\n",
    "letter_lenght = data_lake.load_obj(Length().name + '.pkl')\n",
    "word_count = data_lake.load_obj(Word_Count().name + '.pkl')\n",
    "word_density = letter_lenght/(word_count+1)\n",
    "\n",
    "data_lake.save_obj(word_density, 'word_density.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter_lenght 55556\n",
      "word_count 55556\n",
      "enumeration_presence 55556\n",
      "enumeration_count 55556\n",
      "enumeration_repeated 55556\n",
      "sec_header_presence 55556\n",
      "response_presence 55556\n",
      "cl_sentence_presence 55556\n",
      "df 55556\n"
     ]
    }
   ],
   "source": [
    "#just checking...\n",
    "\n",
    "for f in features:\n",
    "    print(f.name, len(data_lake.load_obj(f.name + '.pkl')))\n",
    "\n",
    "print('df', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToLower(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        return [text.lower() for text in texts]\n",
    "        \n",
    "class TextWithOutStopWords(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        texts_ = []\n",
    "        for text in texts:\n",
    "            text = text.replace('\\n', ' ')\n",
    "            text = \" \".join([i for i in word_tokenize(text) if i not in STOPWORDS])\n",
    "            texts_.append(text)\n",
    "        \n",
    "        return texts_\n",
    "\n",
    "class TextWithOutNonASCII(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        texts_ = []\n",
    "        for text in texts:\n",
    "            text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "            texts_.append(text)\n",
    "        \n",
    "        return texts_\n",
    "    \n",
    "ps = PorterStemmer() \n",
    "class TextStemming(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        texts_ = []\n",
    "        for text in texts:\n",
    "            tokenized_words = word_tokenize(text)\n",
    "            for i, w in enumerate(tokenized_words):\n",
    "                tokenized_words[i] = ps.stem(w)\n",
    "                text = ' '.join(tokenized_words)\n",
    "            \n",
    "            texts_.append(text)\n",
    "        \n",
    "        return texts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLake' object has no attribute 'save_object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-2dc80bbf0553>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata_lake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dataset/train_set/'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLake' object has no attribute 'save_object'"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"T_to_L\", TextToLower()),\n",
    "    (\"T_no_SW\", TextWithOutStopWords()),\n",
    "    (\"T_no_ascii\", TextWithOutNonASCII()),\n",
    "    (\"TS\", TextStemming())#PROBAR TMB CON Lemmatization !!!\n",
    "    ])\n",
    "    \n",
    "f_name = 'text_normalized'\n",
    "texts_t = pipeline.transform(texts)\n",
    "\n",
    "df[f_name] = texts_t\n",
    "data_lake.save_obj(df[f_name], f_name + '.pkl')\n",
    "del df[f_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_feature('letter_lenght')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'letter_lenght distribution')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df.letter_lenght, bins = 1000)\n",
    "plt.xlim(0, 60000)\n",
    "plt.ylabel('density')\n",
    "plt.title('letter_lenght distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_.letter_lenght > 35000 => 4.18\n",
      "df_.letter_lenght > 35000 => ['RL' 'UU']\n"
     ]
    }
   ],
   "source": [
    "threshold = 35000\n",
    "msg = \"df_.letter_lenght > \" + str(threshold) + \" =>\"\n",
    "print(msg, round(len(df[df.letter_lenght > threshold]) / len(df) * 100, 2))\n",
    "print(msg, df[df.letter_lenght > threshold].category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing outliers\n",
    "df = df[df.letter_lenght < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_table = df[df.path == 'source/dataset/train_set/']\n",
    "df_test_table = df[df.path == 'source/dataset/test_set/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train & test dataset presentan las mismas categorias\n"
     ]
    }
   ],
   "source": [
    "# SHOULD REMOVE ACAK CATEGORY !!!\n",
    "#\n",
    "\n",
    "train_categories = df_train_table.category.unique()\n",
    "test_categories = df_test_table.category.unique()\n",
    "\n",
    "if len(train_categories) < len(test_categories):\n",
    "    print('categorias que no se puede entrenar:')\n",
    "    cat_to_remove = [c for c in test_categories if c not in train_categories]\n",
    "    print(cat_to_remove)\n",
    "\n",
    "    #removing categories that are not in the train data set\n",
    "    for c in cat_to_remove:\n",
    "        df = df[df.category != c]\n",
    "        df_train_table = df[df.path == 'source/dataset/train_set/']\n",
    "        df_test_table = df[df.path == 'source/dataset/test_set/']\n",
    "else:\n",
    "    print('train & test dataset presentan las mismas categorias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['letter_lenght']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving cleaned dataframe !!\n",
    "#\n",
    "\n",
    "df.to_csv('../../source/features/v3/df-cleaned.csv')\n",
    "#data_lake.save_obj(df, 'df-cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_feature(f_name)\n",
    "train_x = df[df.path == 'source/dataset/train_set/'][f_name]\n",
    "valid_x = df[df.path == 'source/dataset/test_set/'][f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting CountVectorizer...\n",
      "fitting finished - time:  124.39757490158081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count Vectors as features\n",
    "########################################\n",
    "\n",
    "print(\"fitting CountVectorizer...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('count_vect_config.txt')\n",
    "\n",
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer=config['analyzer'],\n",
    "                             token_pattern=config['token_pattern'],\n",
    "                             ngram_range=config['ngram_range'])\n",
    "count_vect.fit(df[f_name])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "#saving matrices\n",
    "data_lake.save_npz(xvalid_count, \"xvalid_count.npz\")\n",
    "data_lake.save_npz(xtrain_count, \"xtrain_count.npz\")\n",
    "data_lake.save_obj(count_vect, \"count_vect.pkl\")\n",
    "\n",
    "print(\"fitting finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting word level tf-idf...\n",
      "fitting finished - time:  47.41982913017273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "###################\n",
    "\n",
    "print(\"fitting word level tf-idf...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('tf_idf_word_vect_config.txt')\n",
    "\n",
    "#TODO: probar otros valores para max_features, 1D uso 50.000\n",
    "tfidf_vect = TfidfVectorizer(analyzer=config['analyzer'],\n",
    "                             token_pattern=config['token_pattern'],\n",
    "                             max_features=config['max_features'])\n",
    "tfidf_vect.fit(df[f_name])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "#saving matrices\n",
    "data_lake.save_npz(xvalid_tfidf, \"xvalid_tfidf.npz\")\n",
    "data_lake.save_npz(xtrain_tfidf, \"xtrain_tfidf.npz\")\n",
    "\n",
    "print(\"fitting finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting ngram level tf-idf...\n",
      "fitting finished - time:  246.00837469100952\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n-gram level tf-idf \n",
    "####################\n",
    "\n",
    "print(\"fitting ngram level tf-idf...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('tf_idf_n_gram_vect_config.txt')\n",
    "\n",
    "#TODO: probar otros valores para max_features, 1D uso 50.000\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer=config['analyzer'],\n",
    "                                   token_pattern=config['token_pattern'],\n",
    "                                   ngram_range=config['ngram_range'],\n",
    "                                   max_features=config['max_features'])\n",
    "tfidf_vect_ngram.fit(df[f_name])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "#saving matrices\n",
    "data_lake.save_npz(xvalid_tfidf_ngram, \"xvalid_tfidf_ngram.npz\")\n",
    "data_lake.save_npz(xtrain_tfidf_ngram, \"xtrain_tfidf_ngram.npz\")\n",
    "data_lake.save_obj(tfidf_vect_ngram, \"tfidf_vect_ngram.pkl\")\n",
    "\n",
    "print(\"fitting finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting characters level tf-idf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting finished - time:  443.69289684295654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# characters level tf-idf\n",
    "#########################\n",
    "\n",
    "print(\"fitting characters level tf-idf...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('tf_idf_char_vect_config.txt')\n",
    "\n",
    "#TODO: probar otros valores para max_features, 1D uso 50.000\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer=config['analyzer'],\n",
    "                                         token_pattern=config['token_pattern'],\n",
    "                                         ngram_range=config['ngram_range'],\n",
    "                                         max_features=config['max_features'])\n",
    "tfidf_vect_ngram_chars.fit(df[f_name])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) \n",
    "\n",
    "#saving matrices\n",
    "data_lake.save_npz(xvalid_tfidf_ngram_chars, \"xvalid_tfidf_ngram_chars\" + \".npz\")\n",
    "data_lake.save_npz(xtrain_tfidf_ngram_chars, \"xtrain_tfidf_ngram_chars\" + \".npz\")\n",
    "\n",
    "print(\"fitting finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LDA Model...\n",
      "training finished - time:  899.3639438152313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Topic Models as features\n",
    "####################################\n",
    "\n",
    "print(\"training LDA Model...\")\n",
    "time_start = time.time()\n",
    "\n",
    "config = data_lake.load_config('lda_config.txt')\n",
    "\n",
    "# train a LDA Model\n",
    "#TODO: SHOULD TRY VALUES FOR n_components !!!\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=config['n_components'],\n",
    "                                                    learning_method=config['learning_method'],\n",
    "                                                    max_iter=config['max_iter'])\n",
    "X_topics = lda_model.fit_transform(xtrain_tfidf_ngram)\n",
    "\n",
    "data_lake.save_obj(X_topics, 'X_topics' + '.pkl')\n",
    "data_lake.save_obj(lda_model, 'lda_model' + '.pkl')\n",
    "\n",
    "print(\"training finished - time: \", time.time() - time_start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
