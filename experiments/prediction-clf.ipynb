{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy import sparse\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import textblob, string, xgboost\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "punctuation_without_dot = list(filter(lambda x: x != '.' or x != ',' , string.punctuation))\n",
    "STOPWORDS = list(set(stopwords.words('english'))) + punctuation_without_dot\n",
    "#WARNING ! ! ! \n",
    "#There are several known issues with ‘english’ and you should consider an alternative.\n",
    "#-> https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words\n",
    "\n",
    "import logging\n",
    "logging.config.fileConfig('logger.conf')\n",
    "logger = logging.getLogger('simpleLogger')\n",
    "#use -> logger.info('some log comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DataLake:\n",
    "    \n",
    "    path = 'features/'\n",
    "    \n",
    "    def __init__(self, version):\n",
    "        self.version = version \n",
    "        self.version_path = self.path + version + \"/\"\n",
    "        \n",
    "        #create version folder if not exists\n",
    "        if not os.path.exists(self.path + self.version):\n",
    "            try:\n",
    "                os.makedirs(self.path + self.version)\n",
    "            except OSError as e:\n",
    "                if e.errno != errno.EEXIST:\n",
    "                    raise\n",
    "    \n",
    "    def save_obj(self, obj, file_name):\n",
    "        with open(self.version_path + file_name, 'wb') as output:  # Overwrites any existing file.\n",
    "            pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def save_npz(self, obj, file_name):\n",
    "        sparse.save_npz(self.version_path + file_name, obj)\n",
    "    \n",
    "    def load_npz(self, file_name):\n",
    "        return sparse.load_npz(self.version_path + file_name)\n",
    "    \n",
    "    def load_obj(self, obj_name):\n",
    "        with open(self.version_path + obj_name, 'rb') as input:\n",
    "            return pickle.load(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lake = DataLake(version='v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(feature, load_version=None):\n",
    "\n",
    "    if load_version is None:\n",
    "        data_lake_ = data_lake\n",
    "    else:\n",
    "        data_lake_ = DataLake(version=load_version)\n",
    "    \n",
    "    df[feature] = data_lake_.load_obj(feature + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_lake.load_obj('df-cleaned.pkl')\n",
    "df_train_table = df[df.path == 'dataset/train_set/']\n",
    "df_test_table = df[df.path == 'dataset/test_set/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_feature('text_normalized')\n",
    "df_train_table = df[df.path == 'dataset/train_set/']\n",
    "df_test_table = df[df.path == 'dataset/test_set/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_):\n",
    "    train_x = df_['text_normalized'].tolist()\n",
    "    train_y = df_.category.tolist()\n",
    "    return (train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_y = prepare_data(df_train_table)\n",
    "_, valid_y = prepare_data(df_test_table)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Vectors as features\n",
    "########################################\n",
    "\n",
    "xvalid_count = data_lake.load_npz('xvalid_count.npz')\n",
    "xtrain_count = data_lake.load_npz(\"xtrain_count.npz\")\n",
    "count_vect = data_lake.load_obj('count_vect.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "###################\n",
    "\n",
    "xvalid_tfidf = data_lake.load_npz(\"xvalid_tfidf.npz\")\n",
    "xtrain_tfidf = data_lake.load_npz(\"xtrain_tfidf.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram level tf-idf \n",
    "####################\n",
    "\n",
    "xvalid_tfidf_ngram = data_lake.load_npz(\"xvalid_tfidf_ngram.npz\")\n",
    "xtrain_tfidf_ngram = data_lake.load_npz(\"xtrain_tfidf_ngram.npz\")\n",
    "tfidf_vect_ngram = data_lake.load_obj(\"tfidf_vect_ngram.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters level tf-idf\n",
    "#########################\n",
    "\n",
    "xvalid_tfidf_ngram_chars = data_lake.load_npz(\"xvalid_tfidf_ngram_chars\" + \".npz\")\n",
    "xtrain_tfidf_ngram_chars = data_lake.load_npz(\"xtrain_tfidf_ngram_chars\" + \".npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_types = sorted(df.category.unique().tolist())\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, valid_y, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    #get accuracy\n",
    "    accuracy = metrics.accuracy_score(predictions, valid_y)\n",
    "\n",
    "    #get items recall info\n",
    "    recall_info = \"\"\n",
    "    items_recall = recall_score(valid_y, predictions, average=None)\n",
    "    \n",
    "    if len(letter_types) != len(items_recall):\n",
    "        raise Exception('len(letter_types) != len(items_recall) ' + str(len(letter_types)) + ' != '+ str(len(items_recall)))\n",
    "        \n",
    "    #filtered_items_recall = filter(lambda x: x[0] == 'CL' or x[0] == 'RL' , zip(letter_types,items_recall))\n",
    "    filtered_items_recall = zip(letter_types,items_recall)\n",
    "    \n",
    "    for item in filtered_items_recall:\n",
    "        recall_info += str(item)\n",
    "    \n",
    "    msg = \"\\n\" + str(classifier) + \"\\n\" + \"items_recall \" + recall_info + \"\\n\" + \"accuracy_score \" + str(accuracy) + \"\\n\"\n",
    "    logger.info(msg)\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Custom Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_bayes_input(df_,  features, token_counts_matrix = None):\n",
    "    \n",
    "    if len(features) == 0:\n",
    "        raise Exception('len(features) == 0')\n",
    "    \n",
    "    #getting features\n",
    "    y = []\n",
    "    data_to_encode = []\n",
    "    for i in range(0, len(df_)):\n",
    "        item = df_.iloc[i]\n",
    "        y.append(item.category)\n",
    "        \n",
    "        item_to_encode = []\n",
    "        for f in features:\n",
    "            item_to_encode.append(item[f])\n",
    "        \n",
    "        data_to_encode.append(item_to_encode)\n",
    "\n",
    "    #encoding features\n",
    "    enc = OneHotEncoder(handle_unknown='error')\n",
    "    enconded_data = enc.fit_transform(data_to_encode)\n",
    "    \n",
    "    #appending matrix to encoded features\n",
    "    token_counts_matrix_log = 'token_counts_matrix_log is None'\n",
    "    if token_counts_matrix is not None:\n",
    "        enconded_data = hstack([enconded_data, token_counts_matrix])\n",
    "        token_counts_matrix_log = 'token_counts_matrix_log is not None'\n",
    "    \n",
    "    #encoding target\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    y = encoder.fit_transform(y)\n",
    "    \n",
    "    msg = \"\\n\" + \"features:\" + \"\\n\" + str(features) + \"\\n\" + token_counts_matrix_log\n",
    "    logger.info(msg)\n",
    "\n",
    "    return (enconded_data, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "features = features + ['cl_sentence_presence','cl_sentence_presence_2']\n",
    "features = features + ['acak_sentence_presence_1']\n",
    "features = features + ['fl_sentence_presence_4']\n",
    "\n",
    "for f in features:\n",
    "    load_feature(f, load_version='v2')\n",
    "    \n",
    "features = []\n",
    "features = ['sec_header_presence','response_presence']\n",
    "features = features + ['enumeration_presence','enumeration_repeated']\n",
    "\n",
    "for f in features:\n",
    "    load_feature(f, load_version='v3')\n",
    "    \n",
    "df_train_table = df[df.path == 'dataset/train_set/']\n",
    "df_test_table = df[df.path == 'dataset/test_set/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, custom fetures:  \n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "items_recall ('AK', 0.5217391304347826)('AR', 1.0)('CL', 0.9903536977491961)('ER', 0.18181818181818182)('FL', 1.0)('ND', 1.0)('RL', 0.997093023255814)('TM', 0.75)('UR', 1.0)('UU', 0.3717948717948718)\n",
      "accuracy_score 0.9198266522210184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enconded_train_data, train_y = map_to_bayes_input(df_train_table, features, xtrain_count)\n",
    "enconded_valid_data, valid_y = map_to_bayes_input(df_test_table, features, xvalid_count)\n",
    "\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "results = train_model(ensemble.RandomForestClassifier(), enconded_train_data, train_y, enconded_valid_data, valid_y)\n",
    "print(\"NB, custom fetures: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, custom fetures:  \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "items_recall ('AK', 0.0)('AR', 0.975)('CL', 0.9935691318327974)('ER', 0.0)('FL', 0.9801980198019802)('ND', 0.0)('RL', 0.9767441860465116)('TM', 0.0)('UR', 1.0)('UU', 0.11538461538461539)\n",
      "accuracy_score 0.866738894907909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enconded_train_data, train_y = map_to_bayes_input(df_train_table, features, xtrain_count)\n",
    "enconded_valid_data, valid_y = map_to_bayes_input(df_test_table, features, xvalid_count)\n",
    "\n",
    "results = train_model(naive_bayes.MultinomialNB(), enconded_train_data, train_y, enconded_valid_data, valid_y)\n",
    "print(\"NB, custom fetures: \", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "items_recall ('AK', 0.0)('AR', 0.975)('CL', 0.9935691318327974)('ER', 0.0)('FL', 0.9702970297029703)('ND', 0.0)('RL', 0.9767441860465116)('TM', 0.0)('UR', 1.0)('UU', 0.07692307692307693)\n",
      "accuracy_score 0.8624052004333694\n",
      "\n",
      "NB, WordLevel TF-IDF:  \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "items_recall ('AK', 0.043478260869565216)('AR', 0.975)('CL', 0.9517684887459807)('ER', 0.0)('FL', 1.0)('ND', 0.0)('RL', 0.9651162790697675)('TM', 0.0)('UR', 1.0)('UU', 0.16666666666666666)\n",
      "accuracy_score 0.8559046587215602\n",
      "\n",
      "NB, N-Gram Vectors:  \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "items_recall ('AK', 0.4782608695652174)('AR', 1.0)('CL', 0.9807073954983923)('ER', 0.18181818181818182)('FL', 1.0)('ND', 0.0)('RL', 0.9709302325581395)('TM', 0.0)('UR', 1.0)('UU', 0.21794871794871795)\n",
      "accuracy_score 0.886240520043337\n",
      "\n",
      "NB, CharLevel Vectors:  \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "items_recall ('AK', 0.13043478260869565)('AR', 0.9)('CL', 0.9614147909967846)('ER', 0.0)('FL', 1.0)('ND', 0.0)('RL', 0.9709302325581395)('TM', 0.0)('UR', 1.0)('UU', 0.15384615384615385)\n",
      "accuracy_score 0.8591549295774648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"NB, WordLevel TF-IDF: \", accuracy) \n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print(\"NB, N-Gram Vectors: \", accuracy) \n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, valid_y)\n",
    "print(\"NB, CharLevel Vectors: \", accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-8dcbc8ccdb40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SVM on Ngram Level TF IDF Vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain_tfidf_ngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxvalid_tfidf_ngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"SVM, N-Gram Vectors: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-58540299522f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, feature_vector_train, label, feature_vector_valid, valid_y, is_neural_net)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vector_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_neural_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# fit the training dataset on the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# predict the labels on validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                 random_seed)\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/svm/_libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"base matrix class for compressed row and column oriented matrices\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y)\n",
    "print( \"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  \n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "items_recall ('AK', 0.5217391304347826)('AR', 1.0)('CL', 0.9871382636655949)('ER', 0.09090909090909091)('FL', 1.0)('ND', 1.0)('RL', 0.9941860465116279)('TM', 0.75)('UR', 1.0)('UU', 0.38461538461538464)\n",
      "accuracy_score 0.9176598049837487\n",
      "\n",
      "RF, WordLevel TF-IDF:  \n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "items_recall ('AK', 0.4782608695652174)('AR', 0.925)('CL', 0.9903536977491961)('ER', 0.7272727272727273)('FL', 1.0)('ND', 1.0)('RL', 0.9912790697674418)('TM', 0.75)('UR', 1.0)('UU', 0.5128205128205128)\n",
      "accuracy_score 0.9317443120260022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count, valid_y)\n",
    "print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
